{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39afe785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Software\\environments\\simple_chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd \n",
    "from typing import List, Dict \n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import time \n",
    "import xmltodict\n",
    "from datasets import load_dataset \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbc8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"../data/\")\n",
    "FINETUNING_DIR = OUTPUT_DIR / \"finetuning\"\n",
    "RAG_DIR = OUTPUT_DIR / \"rag\"\n",
    "\n",
    "FINETUNING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88fe0c",
   "metadata": {},
   "source": [
    "## Dataset For Finetuning : load and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a87576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mediqationqa(file_path):\n",
    "\tdf = pd.read_excel(file_path)\n",
    "\t# convert to finetuning format\n",
    "\tfinetuning_data = []\n",
    "\tfor _, row in df.iterrows():\n",
    "\t\t# clean and format \n",
    "\t\tquestion = str(row['Question']).strip()\n",
    "\t\tanswer = str(row['Answer']).strip()\n",
    "\t\t\n",
    "\t\tif question and answer and question != 'nan' and answer != 'nan':\n",
    "\t\t\tfinetuning_data.append({\n",
    "\t\t\t\t\t\t\"instruction\" : question,\n",
    "\t\t\t\t\t\t\"output\": answer,\n",
    "\t\t\t})\n",
    "\t\n",
    "\treturn finetuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc084945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MedQuAd(file_path=\"../data/finetuning/mplusdrugs_with_answers.csv\"):\n",
    "\tdf = pd.read_csv(file_path)\n",
    "\tprint(\"Original samples:\", len(df))\n",
    "\n",
    "\t# STEP 1: sample 3500 stratify\n",
    "\tTARGET_SIZE = 3500\n",
    "\tdf_sampled, _ = train_test_split(\n",
    "\t\tdf,\n",
    "\t\ttrain_size=TARGET_SIZE,\n",
    "\t\tstratify=df[\"question_type\"],\n",
    "\t\trandom_state=42\n",
    "\t)\n",
    "\n",
    "\t# STEP 2: now split again into train/test (80:20) while keeping stratification\n",
    "\ttrain_df, test_df = train_test_split(\n",
    "\t\tdf_sampled,\n",
    "\t\ttest_size=0.2,\n",
    "\t\tstratify=df_sampled[\"question_type\"],\n",
    "\t\trandom_state=42\n",
    "\t)\n",
    "\n",
    "\tprint(\"Train distrib:\")\n",
    "\tprint(train_df[\"question_type\"].value_counts())\n",
    "\tprint(\"\\nTest distrib:\")\n",
    "\tprint(test_df[\"question_type\"].value_counts())\n",
    "\n",
    "\t# convert to finetuning format\n",
    "\tdef convert(df):\n",
    "\t\tdata = []\n",
    "\t\tfor _, row in df.iterrows():\n",
    "\t\t\tq = str(row[\"question\"]).strip()\n",
    "\t\t\ta = str(row[\"answer\"]).strip()\n",
    "\t\t\tif q and a:\n",
    "\t\t\t\tdata.append({\"instruction\": q, \"output\": a})\n",
    "\t\treturn data\n",
    "\n",
    "\treturn convert(train_df), convert(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6476bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_questions(finetuning_data):\n",
    "\tseen = set()\n",
    "\tunique_data = []\n",
    "\n",
    "\tfor item in finetuning_data:\n",
    "\t\tq = item[\"instruction\"].strip().lower()\n",
    "\n",
    "\t\tif q not in seen:\n",
    "\t\t\tseen.add(q)\n",
    "\t\t\tunique_data.append(item)\n",
    "\n",
    "\tprint(f\"Removed duplicates. Before: {len(finetuning_data)}, After: {len(unique_data)}\")\n",
    "\treturn unique_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578b44",
   "metadata": {},
   "source": [
    "## Dataset for Knowledge Base for simple RAG using FAISS : DailyMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77eaf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_dailymed_drug_info(drug_name: str, verbose=True):\n",
    "\t\"\"\"\n",
    "\tFinal version: Fetch DailyMed XML using API v1 with correct drug filtering.\n",
    "\t- Step 1: Search SPLs by exact drug name (v1)\n",
    "\t- Step 2: Select latest SPL version\n",
    "\t- Step 3: Download SPL XML via v2 endpoint\n",
    "\t\"\"\"\n",
    "\n",
    "\t# -------------------------\n",
    "\t# STEP 1 ‚Äî QUERY API v1\n",
    "\t# -------------------------\n",
    "\turl = f\"https://dailymed.nlm.nih.gov/dailymed/services/v1/drugname/{drug_name}/human/spls.json\"\n",
    "\t\n",
    "\ttry:\n",
    "\t\tresponse = requests.get(url, timeout=10)\n",
    "\t\tresponse.raise_for_status()\n",
    "\t\tdata = response.json()\n",
    "\texcept Exception as e:\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"‚ùå Error searching SPL for {drug_name}: {e}\")\n",
    "\t\treturn None\n",
    "\n",
    "\trows = data.get(\"DATA\", [])\n",
    "\tif not rows:\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"‚ö† No SPL entries found for drug '{drug_name}'\")\n",
    "\t\treturn None\n",
    "\n",
    "\t# -------------------------\n",
    "\t# STEP 2 ‚Äî Pick latest SPL\n",
    "\t# -------------------------\n",
    "\tdef parse_date(date_str):\n",
    "\t\ttry:\n",
    "\t\t\treturn datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "\t\texcept:\n",
    "\t\t\treturn datetime.min\n",
    "\n",
    "\t# rows format:\n",
    "\t# [ SETID, TITLE, SPL_VERSION, PUBLISHED_DATE ]\n",
    "\tlatest_row = max(rows, key=lambda r: parse_date(r[3]))\n",
    "\n",
    "\tsetid = latest_row[0]\n",
    "\ttitle = latest_row[1]\n",
    "\tpublished = latest_row[3]\n",
    "\n",
    "\tif verbose:\n",
    "\t\tprint(f\"‚úÖ Found SPL for {drug_name}\")\n",
    "\t\tprint(f\"   Title     : {title}\")\n",
    "\t\tprint(f\"   SETID     : {setid}\")\n",
    "\t\tprint(f\"   Published : {published}\")\n",
    "\n",
    "\t# -------------------------\n",
    "\t# STEP 3 ‚Äî DOWNLOAD XML\n",
    "\t# -------------------------\n",
    "\txml_url = f\"https://dailymed.nlm.nih.gov/dailymed/services/v2/spls/{setid}.xml\"\n",
    "\n",
    "\ttry:\n",
    "\t\txml_resp = requests.get(xml_url, timeout=15)\n",
    "\t\txml_resp.raise_for_status()\n",
    "\t\txml_dict = xmltodict.parse(xml_resp.content)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"üì¶ Successfully fetched XML for {drug_name}\")\n",
    "\t\treturn xml_dict\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"‚ùå Failed to fetch XML for SETID {setid}: {e}\")\n",
    "\t\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eea91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found SPL for Ibuprofen\n",
      "   Title     : IBUPROFEN CAPSULE, LIQUID FILLED [CHAIN DRUG MARKETING ASSOCIATION INC.]\n",
      "   SETID     : 4573753d-117b-a6f2-e063-6394a90ae2f9\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Ibuprofen\n"
     ]
    }
   ],
   "source": [
    "data = fetch_dailymed_drug_info(\"Ibuprofen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bea97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to clean html tag from text \n",
    "import re\n",
    "\n",
    "def strip_tags(xml_string):\n",
    "\t# hapus semua tag <...>\n",
    "\ttext = re.sub(r\"<[^>]+>\", \" \", xml_string)\n",
    "\t# normalisasi whitespace\n",
    "\ttext = re.sub(r\"\\s+\", \" \", text)\n",
    "\treturn text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54589a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=400, overlap=100):\n",
    "\t# breakdown to sentences\n",
    "\tsentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\t\n",
    "\tchunks = []\n",
    "\tcurrent_chunk = []\n",
    "\tcurrent_length = 0\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence_len = len(sentence)\n",
    "\n",
    "\t\t# if adding sentence exceeds the max_length\n",
    "\t\tif current_length + sentence_len > max_length:\n",
    "\t\t\tchunks.append(\" \".join(current_chunk))\n",
    "\t\t\t# start new chunk with overlap\n",
    "\t\t\toverlap_sentences = current_chunk[-3:]  # overlap by last 3 sentences\n",
    "\t\t\tcurrent_chunk = overlap_sentences.copy()\n",
    "\t\t\tcurrent_length = sum(len(s) for s in current_chunk)\n",
    "\t\t\n",
    "\t\tcurrent_chunk.append(sentence)\n",
    "\t\tcurrent_length += sentence_len\n",
    "\t\n",
    "\t# save last chunk\n",
    "\tif current_chunk:\n",
    "\t\tchunks.append(\" \".join(current_chunk))\n",
    "\t\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7d8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_drug_sections(xml_dict, drug_name):\n",
    "\tif not xml_dict:\n",
    "\t\treturn []\n",
    "\t\n",
    "\tsection_of_interest = {\n",
    "\t\t\t'dosage' : ['DOSAGE', 'DOSAGE AND ADMINISTRATION'], \n",
    "\t\t\t'contraindications': ['CONTRAINDICATIONS SECTION', 'CONTRAINDICATIONS'],\n",
    "\t\t\t'side_effects': ['ADVERSE REACTIONS', 'SIDE EFFECTS'],\n",
    "\t\t\t'mechanism': ['MECHANISM OF ACTION', 'CLINICAL PHARMACOLOGY'],\n",
    "\t\t\t'warnings': ['WARNINGS', 'WARNINGS AND PRECAUTIONS'], \n",
    "\t\t\t'indications': ['INDICATIONS', 'INDICATIONS AND USAGE'], \n",
    "\t\t\t'interactions': ['DRUG INTERACTIONS'],\n",
    "\t\t\t'overdosage': ['OVERDOSAGE']\n",
    "\t}\n",
    "\n",
    "\trag_chunks = []\n",
    "\n",
    "\ttry:\n",
    "\t\tcomponents = (\n",
    "\t\t\txml_dict.get(\"document\", {})\n",
    "\t\t\t\t\t\t\t.get(\"component\", {})\n",
    "\t\t\t\t\t\t\t.get(\"structuredBody\", {})\n",
    "\t\t\t\t\t\t\t.get(\"component\", [])\n",
    "\t\t)\n",
    "\n",
    "\t\t# fallback if structureBody is not present\n",
    "\t\t# if not components:\n",
    "\t\t# \tprint(\"Falling back to direct components\")\n",
    "\t\t# \tcomponents = xml_dict.get(\"document\", {}).get(\"component\", [])\n",
    "\n",
    "\t\t# components should be a list \n",
    "\t\tif isinstance(components, dict):\n",
    "\t\t\tcomponents = [components]\n",
    "\t\t\n",
    "\t\tfor comp in components:\n",
    "\t\t\tsection = comp.get(\"section\", {})\n",
    "\t\t\tif not section:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# extract section title \n",
    "\t\t\tcode = section.get(\"code\", {})\n",
    "\t\t\ttitle = code.get(\"@displayName\", \"\").upper()\n",
    "\n",
    "\t\t\t# extract raw text (it can be list or dict)\n",
    "\t\t\ttext = section.get(\"text\", \"\")\n",
    "\t\t\tif isinstance(text, dict):\n",
    "\t\t\t\t# convert HTML-ish XML content to string\n",
    "\t\t\t\ttext = xmltodict.unparse({\"text\": text}, pretty=False)\n",
    "\n",
    "\t\t\tif isinstance(text, list):\n",
    "\t\t\t\ttext = \"\\n\".join(str(t) for t in text)\n",
    "\t\t\t\n",
    "\t\t\ttext = str(text).strip()\n",
    "\n",
    "\t\t\t# clean tags \n",
    "\t\t\ttext = strip_tags(text)\n",
    "\n",
    "\t\t\t# match section with interest list\n",
    "\t\t\tfor category, keywords in section_of_interest.items():\n",
    "\t\t\t\tif any(k in title for k in keywords):\n",
    "\t\t\t\t\tif len(text) > 50:\n",
    "\t\t\t\t\t\tchunks = chunk_text(text, max_length=400, overlap=100)\n",
    "\t\t\t\t\t\tfor i, ch in enumerate(chunks):\n",
    "\t\t\t\t\t\t\trag_chunks.append({\n",
    "\t\t\t\t\t\t\t\t\t\"drug_name\": drug_name,\n",
    "\t\t\t\t\t\t\t\t\t\"category\": category,\n",
    "\t\t\t\t\t\t\t\t\t\"section_title\": f\"{title} (chunk {i+1})\",\n",
    "\t\t\t\t\t\t\t\t\t\"text\": ch,\n",
    "\t\t\t\t\t\t\t\t\t\"source\": \"DailyMed\"\n",
    "                })\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\tbreak # stop checking other categories once matched\n",
    "\t\t\t\n",
    "\t\treturn rag_chunks\n",
    "\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error extracting sections for {drug_name}: {e}\")\n",
    "\t\treturn []\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ee8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found SPL for Acetaminophen\n",
      "   Title     : ACETAMINOPHEN SOLUTION [PAI HOLDINGS, LLC DBA PAI PHARMA]\n",
      "   SETID     : fac4e0c6-684f-45a1-99f2-de4de4017cc8\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Acetaminophen\n",
      "4\n",
      "Acetaminophen\n",
      "indications\n",
      "INDICATIONS & USAGE SECTION (chunk 1)\n",
      "‚Ä¢ for the temporary relief of minor aches and pains due to ‚Ä¢ for the minor pain from arthritis ‚Ä¢ and to reduce fever headache muscular aches backache sore throat flu the common cold toothache premenstrual and menstrual cramps\n",
      "\n",
      "Acetaminophen\n",
      "dosage\n",
      "OVERDOSAGE SECTION (chunk 1)\n",
      "Overdose warning: Taking more than the recommended dose (overdose) may cause liver damage. In case of overdose, get medical help or contact a Poison Control Center right away. (1-800-222-1222). Quick medical attention is critical for adults as well as children even if you do not notice any signs or symptoms.\n",
      "\n",
      "Acetaminophen\n",
      "dosage\n",
      "DOSAGE & ADMINISTRATION SECTION (chunk 1)\n",
      "\n",
      "\n",
      "Acetaminophen\n",
      "dosage\n",
      "DOSAGE & ADMINISTRATION SECTION (chunk 2)\n",
      "overdose warning do not take more than directed (see ) dose product from the single dose cup the product is packaged in mL=milliliter age dose adults and children 12 years of age and over 20 mL (640 mg) every 4 to 6 hours not to exceed 6 doses in a 24-hour period children 6 to under 12 years of age 10 mL (320 mg) every 4 hours not to exceed 5 doses in a 24-hour period children 4 to under 6 years of age 7.5 mL (240 mg) every 4 hours not to exceed 5 doses in a 24-hour period children 2 to under 4 years of age 5 mL (160 mg) every 4 hours not to exceed 5 doses in a 24-hour period children under 2 years of age consult a doctor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xml_dict = fetch_dailymed_drug_info(\"Acetaminophen\")\n",
    "chunks = extract_drug_sections(xml_dict, \"Acetaminophen\")\n",
    "\n",
    "print(len(chunks))\n",
    "for c in chunks:\n",
    "\tprint(c[\"drug_name\"])\n",
    "\tprint(c[\"category\"])\n",
    "\tprint(c[\"section_title\"])\n",
    "\tprint(c[\"text\"])\n",
    "\tprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11674ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DRUGS =[\n",
    "\t\"Aspirin\", \"Ibuprofen\", \"Acetaminophen\",\n",
    "\t\"Amoxicillin\", \"Azithromycin\", \"Ciprofloxacin\",\n",
    "\t\"Metformin\", \"Atorvastatin\", \"Lisinopril\",\n",
    "\t\"Omeprazole\", \"Levothyroxine\", \"Albuterol\",\n",
    "\t\"Gabapentin\", \"Sertraline\", \"Losartan\",\n",
    "\t\"Vitamin D\", \"Vitamin B12\", \"Vitamin C\", \n",
    "]\n",
    "\n",
    "\n",
    "def build_rag_knowledge_base():\n",
    "\tprint(\"Building RAG Knowledge Base from DailyMed...\")\n",
    "\n",
    "\tall_rag_chunks = []\n",
    "\tfor drug in TARGET_DRUGS:\n",
    "\t\ttime.sleep(1)  # to respect API rate limits\n",
    "\t\t# fetch xml data\n",
    "\t\txml_dict = fetch_dailymed_drug_info(drug)\n",
    "\n",
    "\t\tif not xml_dict:\n",
    "\t\t\tprint(f\"No dailymed data for {drug}\")\n",
    "\t\t\tcontinue \n",
    "\n",
    "\t\t# extract section \n",
    "\t\tchunks = extract_drug_sections(xml_dict, drug)\n",
    "\t\tprint(f\" ‚úì Extracted {len(chunks)} chunks\")\n",
    "\n",
    "\t\tall_rag_chunks.extend(chunks)\n",
    "\n",
    "\t\ttime.sleep(1)  # to respect API rate limits\n",
    "\n",
    "\tprint(f\"‚úì Total RAG chunks collected: {len(all_rag_chunks)}\")\n",
    "\treturn all_rag_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d562169",
   "metadata": {},
   "source": [
    "### Save finetuning dataset and rag dataset\n",
    "if the two cell below run more than once it will override curren json and jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8842e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_finetuning_data(train_data, test_data):\n",
    "\t\tprint(f\"Saving dataset...\")\n",
    "\t\tprint(f\"Train: {len(train_data)} samples\")\n",
    "\t\tprint(f\"Test : {len(test_data)} samples\")\n",
    "\n",
    "\t\t# Shuffle train only (test tidak perlu)\n",
    "\t\ttrain_data = shuffle(train_data, random_state=42)\n",
    "\n",
    "\t\t# Save JSON\n",
    "\t\twith open(FINETUNING_DIR/\"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tjson.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\t\twith open(FINETUNING_DIR/\"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tjson.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\t\t# Save JSONL\n",
    "\t\twith open(FINETUNING_DIR/\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tfor item in train_data:\n",
    "\t\t\t\t\t\tf.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\t\twith open(FINETUNING_DIR/\"test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tfor item in test_data:\n",
    "\t\t\t\t\t\tf.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\t\tprint(f\"Data saved to {FINETUNING_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401f841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rag_data(rag_chunks):\n",
    "\tprint(\"Saving RAG knowledge base...\")\n",
    "\t\n",
    "\t# save as JSON\n",
    "\twith open(RAG_DIR / \"knowledge_base.json\", 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(rag_chunks, f, indent=2, ensure_ascii=False)\n",
    "\t\n",
    "\t# save as JSONL\n",
    "\twith open(RAG_DIR / \"knowledge_base.jsonl\", 'w', encoding='utf-8') as f:\n",
    "\t\tfor chunk in rag_chunks:\n",
    "\t\t\tf.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\t\n",
    "\t# save statistic \n",
    "\tstats = {\n",
    "\t\t'total_chunks': len(rag_chunks),\n",
    "\t\t'chunks_per_drug': {},\n",
    "\t\t'chunks_per_category': {}\n",
    "\t}\n",
    "\n",
    "\tfor chunk in rag_chunks:\n",
    "\t\tdrug = chunk['drug_name']\n",
    "\t\tcategory = chunk['category']\n",
    "\t\tstats['chunks_per_drug'][drug] = stats['chunks_per_drug'].get(drug, 0) + 1\n",
    "\t\tstats['chunks_per_category'][category] = stats['chunks_per_category'].get(category, 0) + 1\n",
    "\t\n",
    "\twith open(RAG_DIR / \"stats.json\", 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(stats, f, indent=2)\n",
    "\t\n",
    "\tprint(f\"Total chunks : {len(rag_chunks)}\")\n",
    "\tprint(f\"Saved to {RAG_DIR}\")\n",
    "\tfor drug, count in stats['chunks_per_drug'].items():\n",
    "\t\tprint(f\" - {drug}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04b251",
   "metadata": {},
   "source": [
    "### Execute All Function Above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "598f2274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 5000\n",
      "Train distrib:\n",
      "question_type\n",
      "precautions              454\n",
      "side effects             453\n",
      "indication               453\n",
      "usage                    442\n",
      "emergency or overdose    413\n",
      "storage and disposal     401\n",
      "important warning        169\n",
      "severe reaction            9\n",
      "contraindication           6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test distrib:\n",
      "question_type\n",
      "precautions              114\n",
      "side effects             113\n",
      "indication               113\n",
      "usage                    110\n",
      "emergency or overdose    104\n",
      "storage and disposal     100\n",
      "important warning         42\n",
      "severe reaction            2\n",
      "contraindication           2\n",
      "Name: count, dtype: int64\n",
      "Removed duplicates. Before: 3489, After: 3449\n",
      "Removed duplicates. Before: 700, After: 699\n",
      "Saving dataset...\n",
      "Train: 3449 samples\n",
      "Test : 699 samples\n",
      "Data saved to ..\\data\\finetuning\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# load medinfo2019\n",
    "medinfo_train = load_mediqationqa(\"../data/finetuning/MedInfo2019-QA-Medications.xlsx\")\n",
    "\n",
    "# load MedQuAD (stratified)\n",
    "medquad_train, medquad_test = load_MedQuAd(\"../data/finetuning/mplusdrugs_with_answers.csv\")\n",
    "\n",
    "# Merge\n",
    "train_data = medquad_train + medinfo_train\n",
    "test_data = medquad_test\n",
    "\n",
    "# Remove duplicates (opsional)\n",
    "train_data = remove_duplicate_questions(train_data)\n",
    "test_data = remove_duplicate_questions(test_data)\n",
    "\n",
    "# save dataset \n",
    "save_finetuning_data(train_data, test_data)\n",
    "\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7526214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAG Knowledge Base from DailyMed...\n",
      "‚úÖ Found SPL for Aspirin\n",
      "   Title     : ENTERIC COATED ASPIRIN REGULAR STRENGTH (ASPIRIN) TABLET, DELAYED RELEASE [BRYANT RANCH PREPACK]\n",
      "   SETID     : e05140fa-9766-4f97-bffc-e36a01b9d38f\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Aspirin\n",
      " ‚úì Extracted 5 chunks\n",
      "‚úÖ Found SPL for Ibuprofen\n",
      "   Title     : IBUPROFEN CAPSULE, LIQUID FILLED [CHAIN DRUG MARKETING ASSOCIATION INC.]\n",
      "   SETID     : 4573753d-117b-a6f2-e063-6394a90ae2f9\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Ibuprofen\n",
      " ‚úì Extracted 6 chunks\n",
      "‚úÖ Found SPL for Acetaminophen\n",
      "   Title     : ACETAMINOPHEN SOLUTION [PAI HOLDINGS, LLC DBA PAI PHARMA]\n",
      "   SETID     : fac4e0c6-684f-45a1-99f2-de4de4017cc8\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Acetaminophen\n",
      " ‚úì Extracted 4 chunks\n",
      "‚úÖ Found SPL for Amoxicillin\n",
      "   Title     : AMOXICILLIN FOR SUSPENSION [NUCARE PHARMACEUTICALS, INC.]\n",
      "   SETID     : 4528f48d-5bda-b307-e063-6294a90a294c\n",
      "   Published : December 05, 2025\n",
      "üì¶ Successfully fetched XML for Amoxicillin\n",
      " ‚úì Extracted 35 chunks\n",
      "‚úÖ Found SPL for Azithromycin\n",
      "   Title     : AZITHROMYCIN TABLET, FILM COATED [BIONPHARMA INC.]\n",
      "   SETID     : 859bb02c-a4bc-46fa-9fc9-def1219b5dfb\n",
      "   Published : December 05, 2025\n",
      "üì¶ Successfully fetched XML for Azithromycin\n",
      " ‚úì Extracted 8 chunks\n",
      "‚úÖ Found SPL for Ciprofloxacin\n",
      "   Title     : CIPROFLOXACIN (CIPROFOLXACIN) TABLET [ST. MARY'S MEDICAL PARK PHARMACY]\n",
      "   SETID     : 24c43000-7620-3869-e063-6394a90a9631\n",
      "   Published : December 09, 2025\n",
      "üì¶ Successfully fetched XML for Ciprofloxacin\n",
      " ‚úì Extracted 32 chunks\n",
      "‚úÖ Found SPL for Metformin\n",
      "   Title     : METFORMIN TABLET, EXTENDED RELEASE [INGENUS PHARMACEUTICALS, LLC]\n",
      "   SETID     : 49a0b5c2-ebaf-4c4c-905f-dfd1962ac647\n",
      "   Published : December 09, 2025\n",
      "üì¶ Successfully fetched XML for Metformin\n",
      " ‚úì Extracted 3 chunks\n",
      "‚úÖ Found SPL for Atorvastatin\n",
      "   Title     : ATORVASTATIN CALCIUM (ATORVASTATIN) TABLET, FILM COATED [DIRECT RX]\n",
      "   SETID     : 7a3d0c78-f0a7-0706-e053-2991aa0ae9f7\n",
      "   Published : January 11, 2021\n",
      "üì¶ Successfully fetched XML for Atorvastatin\n",
      " ‚úì Extracted 187 chunks\n",
      "‚úÖ Found SPL for Lisinopril\n",
      "   Title     : LISINOPRIL TABLET [CARDINAL HEALTH 107, LLC]\n",
      "   SETID     : 1d0caa63-9ea2-4a97-a23b-900a7c514bc6\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Lisinopril\n",
      " ‚úì Extracted 7 chunks\n",
      "‚ùå Error searching SPL for Omeprazole: HTTPSConnectionPool(host='dailymed.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\n",
      "No dailymed data for Omeprazole\n",
      "‚úÖ Found SPL for Levothyroxine\n",
      "   Title     : LEVOTHYROXINE LIQUID [DESERET BIOLOGICALS, INC.]\n",
      "   SETID     : 1a467f8b-2611-4936-af2b-098e4791d6ae\n",
      "   Published : December 14, 2023\n",
      "üì¶ Successfully fetched XML for Levothyroxine\n",
      " ‚úì Extracted 3 chunks\n",
      "‚úÖ Found SPL for Albuterol\n",
      "   Title     : ALBUTEROL TABLET [BRYANT RANCH PREPACK]\n",
      "   SETID     : 601a18ed-d66e-498f-a804-f1a3330827d4\n",
      "   Published : April 23, 2025\n",
      "üì¶ Successfully fetched XML for Albuterol\n",
      " ‚úì Extracted 49 chunks\n",
      "‚úÖ Found SPL for Gabapentin\n",
      "   Title     : GABAPENTIN TABLET, FILM COATED [CARDINAL HEALTH 107, LLC]\n",
      "   SETID     : b2da7d96-73ad-4f44-8784-160a384a0961\n",
      "   Published : December 10, 2025\n",
      "üì¶ Successfully fetched XML for Gabapentin\n",
      " ‚úì Extracted 9 chunks\n",
      "‚úÖ Found SPL for Sertraline\n",
      "   Title     : SERTRALINE TABLET, FILM COATED [REMEDYREPACK INC.]\n",
      "   SETID     : 7efc1dd5-8671-4bed-9c16-ef6b9f36afc4\n",
      "   Published : October 06, 2025\n",
      "üì¶ Successfully fetched XML for Sertraline\n",
      " ‚úì Extracted 10 chunks\n",
      "‚úÖ Found SPL for Losartan\n",
      "   Title     : LOSARTAN POTASSIUM (LOSARTAN) TABLET [GRANULES PHARMACEUTICALS INC.]\n",
      "   SETID     : ca012edc-923d-4ada-a261-db82b71b3c4f\n",
      "   Published : December 05, 2025\n",
      "üì¶ Successfully fetched XML for Losartan\n",
      " ‚úì Extracted 7 chunks\n",
      "‚ùå Error searching SPL for Vitamin D: 404 Client Error: Not Found for url: https://dailymed.nlm.nih.gov/dailymed/services/v1/drugname/Vitamin%20D/human/spls.json\n",
      "No dailymed data for Vitamin D\n",
      "‚ùå Error searching SPL for Vitamin B12: 404 Client Error: Not Found for url: https://dailymed.nlm.nih.gov/dailymed/services/v1/drugname/Vitamin%20B12/human/spls.json\n",
      "No dailymed data for Vitamin B12\n",
      "‚ùå Error searching SPL for Vitamin C: 404 Client Error: Not Found for url: https://dailymed.nlm.nih.gov/dailymed/services/v1/drugname/Vitamin%20C/human/spls.json\n",
      "No dailymed data for Vitamin C\n",
      "‚úì Total RAG chunks collected: 365\n",
      "Saving RAG knowledge base...\n",
      "Total chunks : 365\n",
      "Saved to ..\\data\\rag\n",
      " - Aspirin: 5 chunks\n",
      " - Ibuprofen: 6 chunks\n",
      " - Acetaminophen: 4 chunks\n",
      " - Amoxicillin: 35 chunks\n",
      " - Azithromycin: 8 chunks\n",
      " - Ciprofloxacin: 32 chunks\n",
      " - Metformin: 3 chunks\n",
      " - Atorvastatin: 187 chunks\n",
      " - Lisinopril: 7 chunks\n",
      " - Levothyroxine: 3 chunks\n",
      " - Albuterol: 49 chunks\n",
      " - Gabapentin: 9 chunks\n",
      " - Sertraline: 10 chunks\n",
      " - Losartan: 7 chunks\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# rag dataset\n",
    "rag_chunks = build_rag_knowledge_base()\n",
    "\t\n",
    "if rag_chunks:\n",
    "\tsave_rag_data(rag_chunks)\n",
    "\n",
    "print(\"Preprocessing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
