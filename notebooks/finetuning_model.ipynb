{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da450e98",
   "metadata": {},
   "source": [
    "### LoRA Fine Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577e8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Software\\environments\\simple_chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    Seq2SeqTrainer,              \n",
    "    Seq2SeqTrainingArguments,    \n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "\tLoraConfig,\n",
    "\tget_peft_model,\n",
    "\tprepare_model_for_kbit_training,\n",
    "\tTaskType\n",
    ")\n",
    "import numpy as np\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3407fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "\t'flan-t5-base': {\n",
    "\t\t'name': 'google/flan-t5-base',\n",
    "\t\t'type': 'seq2seq',\n",
    "\t\t'quantization': None\n",
    "\t}\n",
    "}\n",
    "\n",
    "SELECTED_MODEL = 'flan-t5-base'\n",
    "\n",
    "DATA_DIR = Path('../data/finetuning/')\n",
    "OUTPUT_DIR = Path('../models/drug_qna_lora')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LORA_CONFIG = {\n",
    "\t\t'r': 8,  # LoRA rank (higher = more parameters, but slower) : 4, 8, 16\n",
    "\t\t'lora_alpha': 32,  # LoRA scaling factor\n",
    "\t\t'lora_dropout': 0.05,  # Dropout for LoRA layers\n",
    "\t\t'target_modules': None,  # Will be set based on model\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "\t\t'num_epochs': 5,\n",
    "\t\t'batch_size': 8,  # Adjust based on VRAM : 2, 4, 8\n",
    "\t\t'gradient_accumulation_steps': 2,  # Effective batch size = 16\n",
    "\t\t'per_device_eval_batch_size': 4,\n",
    "\t\t'learning_rate': 1e-4,\n",
    "\t\t'max_length': 256,\n",
    "\t\t'warmup_ratio': 0.03,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b409e2",
   "metadata": {},
   "source": [
    "### Load dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fa279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the finetuning dataset is small, we can use either json or jsonl\n",
    "def load_dataset_from_json(split = 'train'):\n",
    "\tfile_path = DATA_DIR / f'{split}.json'\n",
    "\tif not file_path.exists():\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"Dataset file not found at {file_path}\"\n",
    "\t\t\t\"Please run preprocessing.ipynb first\"\n",
    "\t\t)\n",
    "\t\n",
    "\twith open(file_path, 'r', encoding='utf-8') as f:\n",
    "\t\tdata = json.load(f)\n",
    "\t\n",
    "\tprint(\"Loaded\")\n",
    "\treturn Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8712bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_jsonl(split='train'):\n",
    "\tfile_path = DATA_DIR / f\"{split}.jsonl\"  \n",
    "\t\n",
    "\tif not file_path.exists():\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"Dataset not found at {file_path}. \"\n",
    "\t\t\t\"Please run data preprocessing first.\"\n",
    "\t\t)\n",
    "\t\n",
    "\t# Read JSONL line-by-line\n",
    "\tdata = []\n",
    "\twith open(file_path, 'r', encoding='utf-8') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tdata.append(json.loads(line))\n",
    "\t\n",
    "\tprint(f\"Loaded {len(data)} examples from {split} set\")\n",
    "\treturn Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0620742",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9b624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(model_config):\n",
    "\tprint(f\"Setting up model for {model_config['name']}\")\n",
    "\tmodel_name = model_config['name']\n",
    "\tmodel_type = model_config['type']\n",
    "\t\n",
    "\t# load tokenizer \n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\t\n",
    "\t# Add padding token if missing\n",
    "\t# pad token is used to give every input the same length according to longest input in the batch\n",
    "\tif tokenizer.pad_token is None:\n",
    "\t\ttokenizer.pad_token = tokenizer.eos_token\n",
    "\t\n",
    "\t# setup quantization config if needed\n",
    "\tquantization_config = None\n",
    "\tif model_config['quantization'] == '4bit':\n",
    "\t\tquantization_config = BitsAndBytesConfig(\n",
    "\t\t\tload_in_4bit=True,\n",
    "\t\t\tbnb_4bit_quant_type=\"nf4\",\n",
    "\t\t\tbnb_4bit_compute_dtype=torch.float16,\n",
    "\t\t\tbnb_4bit_use_double_quant=True,\n",
    "\t\t)\n",
    "\t\n",
    "\t# load model \n",
    "\tif model_type == 'seq2seq':\n",
    "\t\tmodel = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "\t\t\tmodel_name,\n",
    "\t\t\tquantization_config=quantization_config,\n",
    "\t\t\tdevice_map='auto',\n",
    "\t\t\ttrust_remote_code=True\n",
    "\t\t)\n",
    "\t\ttask_type = TaskType.SEQ_2_SEQ_LM\n",
    "\t\t# target modules for T5\n",
    "\t\ttarget_modules = ['q', 'v']\n",
    "\telse: # causal LM or auto-regressive LM like GPT-2 , Phi-2\n",
    "\t\tmodel = AutoModelForCausalLM.from_pretrained(\n",
    "\t\t\tmodel_name,\n",
    "\t\t\tquantization_config=quantization_config,\n",
    "\t\t\tdevice_map=\"auto\",\n",
    "\t\t\ttrust_remote_code=True,\n",
    "\t\t\ttorch_dtype=torch.float16\n",
    "\t\t)\n",
    "\t\ttask_type = TaskType.CAUSAL_LM\n",
    "\t\ttarget_modules = ['q_proj', 'v_proj']\n",
    "\t\n",
    "\tprint(f\"Model loaded: {model_name}\")\n",
    "\n",
    "\t# prepare model for k-bit training if quantized\n",
    "\tif quantization_config:\n",
    "\t\tmodel = prepare_model_for_kbit_training(model)\n",
    "\t\tprint(\"Model prepared for k-bit training\")\n",
    "\t\n",
    "\t# setup LoRA config\n",
    "\tlora_config = LoraConfig(\n",
    "\t\tr=LORA_CONFIG['r'],\n",
    "\t\tlora_alpha=LORA_CONFIG['lora_alpha'],\n",
    "\t\ttarget_modules=target_modules,\n",
    "\t\tlora_dropout=LORA_CONFIG['lora_dropout'],\n",
    "\t\tbias=\"none\",\n",
    "\t\ttask_type=task_type\n",
    "\t)\n",
    "\n",
    "\tmodel = get_peft_model(model, lora_config)\n",
    "\n",
    "\t# print trainable parameters\n",
    "\ttrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\ttotal_params = sum(p.numel() for p in model.parameters())\n",
    "\tprint(f\"  Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\tprint(f\"  Total params: {total_params:,}\")\n",
    "\n",
    "\treturn model, tokenizer, model_type\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c4dfd",
   "metadata": {},
   "source": [
    "### Preprocessing model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ec0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using seq2seq model like flan-t5\n",
    "def preprocess_function_seq2seq(examples, tokenizer, max_length):\n",
    "\t# inputs = examples['instruction']\n",
    "\tinputs = [f\"answer the following medication question:\\n{q}\" for q in examples['instruction']]\n",
    "\ttargets = examples['output']\n",
    "\t\n",
    "\t# Tokenize inputs\n",
    "\tmodel_inputs = tokenizer(\n",
    "\t\t\tinputs,\n",
    "\t\t\tmax_length=max_length,\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tpadding='max_length'\n",
    "\t)\n",
    "\t\n",
    "\t# Tokenize targets\n",
    "\twith tokenizer.as_target_tokenizer():\n",
    "\t\tlabels = tokenizer(\n",
    "\t\t\t\ttargets,\n",
    "\t\t\t\tmax_length=max_length,\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\tpadding='max_length'\n",
    "\t\t\t\t# padding=False\n",
    "\t\t)\n",
    "\t\n",
    "\t# Replace padding token id with -100 in labels to ignore padding in loss\n",
    "\tlabels_ids = labels['input_ids']\n",
    "\tfor label_seq in labels_ids:\n",
    "\t\t\tfor i in range(len(label_seq)):\n",
    "\t\t\t\t\tif label_seq[i] == tokenizer.pad_token_id:\n",
    "\t\t\t\t\t\t\tlabel_seq[i] = -100\n",
    "\t\t\t\t\t\t\t\n",
    "\tmodel_inputs['labels'] = labels_ids\n",
    "\t\n",
    "\treturn model_inputs\n",
    "\n",
    "# output structure : \n",
    "# {\n",
    "#   'input_ids': [...],\n",
    "#   'attention_mask': [...],\n",
    "#   'labels': [...]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_causal(examples, tokenizer, max_length):\n",
    "\t\"\"\"\n",
    "\tPreprocess data for Causal LM models (Phi-2, TinyLlama)\n",
    "\t\"\"\"\n",
    "\t# Format: Instruction: {question}\\n\\nAnswer: {answer}\n",
    "\ttexts = []\n",
    "\tfor instruction, output in zip(examples['instruction'], examples['output']):\n",
    "\t\ttext = f\"Instruction: {instruction}\\n\\nAnswer: {output}\"\n",
    "\t\ttexts.append(text)\n",
    "\t\n",
    "\t# Tokenize\n",
    "\tmodel_inputs = tokenizer(\n",
    "\t\ttexts,\n",
    "\t\tmax_length=max_length,\n",
    "\t\ttruncation=True,\n",
    "\t\tpadding='max_length'\n",
    "\t)\n",
    "\t\n",
    "\t# For causal LM, labels are the same as input_ids\n",
    "\tmodel_inputs['labels'] = model_inputs['input_ids'].copy()\n",
    "\t\n",
    "\treturn model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45aa03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, model_type):\n",
    "\tprint(\"Preparing dataset...\")\n",
    "\t\n",
    "\t# load dataset\n",
    "\ttrain_dataset = load_dataset_from_json('train')\n",
    "\ttest_dataset = load_dataset_from_json('test')\n",
    "\t\n",
    "\t# select preprocessing function \n",
    "\tif model_type == 'seq2seq':\n",
    "\t\tpreprocess_fn = lambda x : preprocess_function_seq2seq(x, tokenizer, TRAINING_CONFIG['max_length'])\n",
    "\t\n",
    "\telse: \n",
    "\t\tpreprocess_fn = lambda x : preprocess_function_causal(x, tokenizer, TRAINING_CONFIG['max_length'])\n",
    "\t\n",
    "\t# preprocess\n",
    "\ttrain_dataset = train_dataset.map(\n",
    "\t\tpreprocess_fn, # for every batch data x , apply preprocess_fn\n",
    "\t\tbatched=True, # using batched processing for speed not single sample at a time\n",
    "\t\tremove_columns=train_dataset.column_names\n",
    "\t)\n",
    "\ttest_dataset = test_dataset.map(\n",
    "\t\tpreprocess_fn,\n",
    "\t\tbatched=True,\n",
    "\t\tremove_columns=test_dataset.column_names\n",
    "\t)\n",
    "\n",
    "\tprint(f\"Train dataset: \t{len(train_dataset)} examples\")\n",
    "\tprint(f\"Test dataset: \t{len(test_dataset)} examples\")\n",
    "\n",
    "\treturn train_dataset, test_dataset\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc65316",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca94301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "# load metrics sekali saja\n",
    "rouge_metric = load(\"rouge\")\n",
    "bleu_metric = load(\"bleu\")\n",
    "f1_metric = load(\"f1\")  # token-level F1\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    eval_pred: EvalPrediction(predictions, label_ids)\n",
    "    predictions: generated token IDs (already decoded by Seq2SeqTrainer)\n",
    "    labels: label IDs\n",
    "    \"\"\"\n",
    "    print(\"Computing metrics...\")\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    # predictions are already token IDs from generation\n",
    "    # If predictions are -100 padded, replace them\n",
    "    if isinstance(preds, np.ndarray):\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    \n",
    "    # decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # decode labels; replace -100 with pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Trim both sides\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # BLEU\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "\n",
    "    # F1-score (token-level)\n",
    "    def simple_f1(pred, label):\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        common = set(pred_tokens) & set(label_tokens)\n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            return 0.0\n",
    "        precision = len(common) / len(pred_tokens)\n",
    "        recall = len(common) / len(label_tokens)\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    f1_scores = [simple_f1(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n",
    "    f1_avg = float(np.mean(f1_scores))\n",
    "    \n",
    "    print(\"Metrics computed.\")\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "        \"f1\": f1_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383ec26",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, train_dataset, test_dataset, model_type):\n",
    "\tprint(\"Starting Training...\")\n",
    "\t\n",
    "\t# Training arguments\n",
    "\ttraining_args = Seq2SeqTrainingArguments(\n",
    "\t\toutput_dir=str(OUTPUT_DIR),\n",
    "\t\tnum_train_epochs=TRAINING_CONFIG['num_epochs'],\n",
    "\t\tper_device_train_batch_size=TRAINING_CONFIG['batch_size'],\n",
    "\t\tper_device_eval_batch_size=TRAINING_CONFIG['per_device_eval_batch_size'],\n",
    "\t\tgradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "\t\tlearning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "\n",
    "\t\t# warmup_steps=TRAINING_CONFIG['warmup_steps'],\n",
    "\t\twarmup_ratio=TRAINING_CONFIG['warmup_ratio'],\n",
    "\t\tweight_decay=0.01,\n",
    "    label_smoothing_factor=0.05, # 0.1 , 0.5\n",
    "\n",
    "\t\tlogging_steps=100, # increase to reduce logging frequency : 10, 100\n",
    "\t\teval_strategy=\"epoch\",\n",
    "\t\tsave_strategy=\"epoch\",\n",
    "\t\tsave_total_limit=2,\n",
    "\t\tload_best_model_at_end=True,\n",
    "\t\tmetric_for_best_model=\"rouge1\", # if using rouge1 then greater is better True, if using eval_loss then False\n",
    "\n",
    "\t\tgreater_is_better=True, # because we use rouge1 and the higher the better\n",
    "\t\tfp16=False, # Disable mixed precision to avoid errors\n",
    "\t\tbf16=True, # Use bfloat16 if supported (i used rtx 4060 so i enable this)\n",
    "\t\tpredict_with_generate=True,  # important for seq2seq to generate during evaluation, avoid list of list of float (logits) to decode() \n",
    "\t\tgeneration_max_length=TRAINING_CONFIG['max_length'],\n",
    "\t\toptim=\"adamw_torch\",\n",
    "\t\treport_to=\"none\",  # Disable wandb, tensorboard\n",
    "\t)\n",
    "\t\n",
    "\t# Data collator\n",
    "\tdata_collator = DataCollatorForSeq2Seq(\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tmodel=model,\n",
    "\t\tpadding=True\n",
    "\t)\n",
    "\t\n",
    "\t# Initialize trainer\n",
    "\ttrainer = Seq2SeqTrainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=train_dataset,\t\n",
    "\t\teval_dataset=test_dataset,\n",
    "\t\tdata_collator=data_collator,\n",
    "\t\tcompute_metrics = lambda eval_pred : compute_metrics(eval_pred, tokenizer)\n",
    "\t)\n",
    "\t\n",
    "\t# Train\n",
    "\tprint(\"Training started...\")\n",
    "\ttrainer.train()\n",
    "\tprint(\"Training completed.\")\n",
    "\n",
    "\t# clear cache before evaluate\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t# Evaluate\n",
    "\tprint(\"Evaluating model...\")\n",
    "\teval_results = trainer.evaluate()\n",
    "\tprint(f\"Evaluation results: {eval_results}\")\n",
    "\tfor key, value in eval_results.items():\n",
    "\t\tprint(f\"  {key}: {value:.4f}\")\n",
    "\t\n",
    "\t# save evaluation result\n",
    "\twith open(OUTPUT_DIR / \"evaluation_results.json\", 'w') as f:\n",
    "\t\tjson.dump(eval_results, f, indent=2)\n",
    "\t\n",
    "\t# Save final model\n",
    "\tprint(\"Saving model...\")\n",
    "\ttrainer.save_model(OUTPUT_DIR / \"final\")\n",
    "\ttokenizer.save_pretrained(OUTPUT_DIR / \"final\")\n",
    "\t\n",
    "\tprint(f\"Model saved to {OUTPUT_DIR / 'final'}\")\n",
    "\t\n",
    "\treturn trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7cc7aa",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f91268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model...\n",
      "Selected model : flan-t5-base\n",
      "Model config : {'name': 'google/flan-t5-base', 'type': 'seq2seq', 'quantization': None}\n",
      "Setting up model for google/flan-t5-base\n",
      "Model loaded: google/flan-t5-base\n",
      "  Trainable params: 884,736 (0.36%)\n",
      "  Total params: 248,462,592\n",
      "Preparing dataset...\n",
      "Loaded\n",
      "Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3449 [00:00<?, ? examples/s]c:\\Software\\environments\\simple_chatbot\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3449/3449 [00:02<00:00, 1343.40 examples/s]\n",
      "Map: 100%|██████████| 699/699 [00:00<00:00, 1358.07 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: \t3449 examples\n",
      "Test dataset: \t699 examples\n",
      "Starting Training...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 1:23:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.030700</td>\n",
       "      <td>3.559692</td>\n",
       "      <td>0.110115</td>\n",
       "      <td>0.015623</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>0.008779</td>\n",
       "      <td>0.041341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.735000</td>\n",
       "      <td>3.256807</td>\n",
       "      <td>0.158803</td>\n",
       "      <td>0.053070</td>\n",
       "      <td>0.147189</td>\n",
       "      <td>0.038538</td>\n",
       "      <td>0.060716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.579000</td>\n",
       "      <td>3.121470</td>\n",
       "      <td>0.252317</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.234268</td>\n",
       "      <td>0.097129</td>\n",
       "      <td>0.137150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.508200</td>\n",
       "      <td>3.060488</td>\n",
       "      <td>0.302213</td>\n",
       "      <td>0.193377</td>\n",
       "      <td>0.277246</td>\n",
       "      <td>0.130816</td>\n",
       "      <td>0.181346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.456300</td>\n",
       "      <td>3.042266</td>\n",
       "      <td>0.304649</td>\n",
       "      <td>0.194534</td>\n",
       "      <td>0.278225</td>\n",
       "      <td>0.133254</td>\n",
       "      <td>0.182185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Training completed.\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 13:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Metrics computed.\n",
      "Evaluation results: {'eval_loss': 3.042266368865967, 'eval_rouge1': 0.30464908702148036, 'eval_rouge2': 0.19453441359912327, 'eval_rougeL': 0.27822454534832114, 'eval_bleu': 0.1332543719158346, 'eval_f1': 0.18218516588043282, 'eval_runtime': 827.5791, 'eval_samples_per_second': 0.845, 'eval_steps_per_second': 0.211, 'epoch': 5.0}\n",
      "  eval_loss: 3.0423\n",
      "  eval_rouge1: 0.3046\n",
      "  eval_rouge2: 0.1945\n",
      "  eval_rougeL: 0.2782\n",
      "  eval_bleu: 0.1333\n",
      "  eval_f1: 0.1822\n",
      "  eval_runtime: 827.5791\n",
      "  eval_samples_per_second: 0.8450\n",
      "  eval_steps_per_second: 0.2110\n",
      "  epoch: 5.0000\n",
      "Saving model...\n",
      "Model saved to ..\\models\\drug_qna_lora\\final\n",
      "Testing inference...\n",
      "Question: What is the dosage of Acetaminophen for adults?\n",
      "Answer: Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults. Acetaminophen is used to treat atopic dermatitis (dermatitis) in adults.\n",
      "\n",
      "Question: Who should get Pegfilgrastim Injection and why is it prescribed ?\n",
      "Answer: Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a bacterial infection in the gastrointestinal tract. Pegfilgrastim injection is used to treat a\n",
      "\n",
      "Question: What is the dosage of Paracetamol for adults?\n",
      "Answer: Paracetamol is used to treat a variety of illnesses, including atopic dermatitis, atopic dermatitis, and atopic dermatitis. Paracetamol is used to treat a variety of illnesses, including atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis, atopic dermatitis,\n",
      "\n",
      "Question: How should Trospium be used and what is the dosage ?\n",
      "Answer: Trospium is used to treat a rash. It is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used to treat a rash. Trospium is used\n",
      "\n",
      "Question: What are the side effects of Ibuprofen?\n",
      "Answer: Ibuprofen may cause side effects. Tell your doctor if any of these symptoms are severe or do not go away: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash may be severe. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash may be severe. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash may be severe. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you experience any of these symptoms, call your doctor immediately: rash, rash, or rash. If you\n",
      "\n",
      "Question: When should I not take Amoxicillin?\n",
      "Answer: Amoxicillin is used to treat bacterial infections. It is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used to treat infections caused by bacterial infections. Amoxicillin is used\n",
      "\n",
      "FINE TUNING COMPLETE. Model saved to ..\\models\\drug_qna_lora\\final\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine tuning model...\")\n",
    "print(f\"Selected model : {SELECTED_MODEL}\")\n",
    "print(f\"Model config : {MODEL_CONFIGS[SELECTED_MODEL]}\")\n",
    "\n",
    "model_config = MODEL_CONFIGS[SELECTED_MODEL]\n",
    "\n",
    "model, tokenizer, model_type = setup_model_and_tokenizer(model_config)\n",
    "\n",
    "train_dataset, test_dataset = prepare_dataset(tokenizer, model_type)\n",
    "\n",
    "trainer = train_model(model, tokenizer, train_dataset, test_dataset, model_type)\n",
    "\n",
    "print(f\"FINE TUNING COMPLETE. Model saved to {OUTPUT_DIR / 'final'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b8bf5",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, tokenizer, model_type):\n",
    "\tprint(\"Testing inference...\")\n",
    "\n",
    "\ttest_questions = [\n",
    "\t\t\"What is the dosage of Acetaminophen for adults?\",\n",
    "\t\t\"Who should get Pegfilgrastim Injection and why is it prescribed ?\",\n",
    "\t\t\"What is the dosage of Paracetamol for adults?\",\n",
    "\t\t\"How should Trospium be used and what is the dosage ?\",\n",
    "\t\t\"What are the side effects of Ibuprofen?\",\n",
    "\t\t\"When should I not take Amoxicillin?\",\n",
    "\t]\n",
    "\n",
    "\tmodel.eval()\n",
    "\tfor question in test_questions:\n",
    "\t\tprint(f\"Question: {question}\")\n",
    "\t\tif model_type == 'seq2seq':\n",
    "\t\t\t# T5 style\n",
    "\t\t\tprompt = f\"answer the following medication question:\\n{question}\"\n",
    "\t\t\tinputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\t\t\toutputs = model.generate(\n",
    "\t\t\t\t**inputs, \n",
    "\t\t\t\tmax_length=256,\n",
    "\t\t\t\tnum_beams=4,\n",
    "\t\t\t\trepetition_penalty=1.2,\n",
    "\t\t\t\tno_repeat_ngram_size=3,\n",
    "\t\t\t\tearly_stopping=True,\n",
    "\t\t\t)\n",
    "\t\t\tanswer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t\telse:\n",
    "\t\t\t# Causal LM style\n",
    "\t\t\tprompt = f\"Instruction: {question}\\n\\nAnswer:\"\n",
    "\t\t\tinputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\t\t\toutputs = model.generate(\n",
    "\t\t\t\t\t**inputs,\n",
    "\t\t\t\t\tmax_length=512,\n",
    "\t\t\t\t\ttemperature=0.7,\n",
    "\t\t\t\t\tdo_sample=True,\n",
    "\t\t\t\t\tpad_token_id=tokenizer.pad_token_id\n",
    "\t\t\t)\n",
    "\t\t\tanswer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t\t\t# Extract only the answer part\n",
    "\t\t\tanswer = answer.split(\"Answer:\")[-1].strip()\n",
    "\t\t\n",
    "\t\tprint(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference(model, tokenizer, model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
